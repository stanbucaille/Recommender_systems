{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://itnext.io/what-are-the-top-recommendation-engine-algorithms-used-nowadays-646f588ce639"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative filtering methids are built on a dataset of user/item feedbacks. In this notebook, we will be working on the **goodbooks-10k** dataset, which provides ratings given by users on books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: **goodbooks-10k**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_goodbooks = \"/Users/stanislasbucaille/Desktop/data_bases/data_books/goodbooks-10k/\"\n",
    "ratings = np.loadtxt(path_goodbooks + 'ratings.csv', delimiter=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_int = []\n",
    "for line in ratings:\n",
    "    b, u, r = int(line[0]), int(line[1]), int(line[2])\n",
    "    ratings_int.append([b, u, r])\n",
    "ratings_int = np.array(ratings_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnaries\n",
    "dic_ratings = {}\n",
    "dic_users = {}\n",
    "dic_books = {}\n",
    "\n",
    "for line in ratings_int:\n",
    "    b, u, r = line[0], line[1], line[2]\n",
    "    \n",
    "    # Ratings\n",
    "    dic_ratings[(b, u)] = r\n",
    "    \n",
    "    # Users\n",
    "    if u not in dic_users:\n",
    "        dic_users[u] = [b]\n",
    "    else:\n",
    "        dic_users[u].append(b)\n",
    "    \n",
    "    # Books\n",
    "    if b not in dic_books:\n",
    "        dic_books[b] = [u]\n",
    "    else:\n",
    "        dic_books[b].append(u)\n",
    "\n",
    "# Create lists\n",
    "list_users = [u for u in dic_users]\n",
    "list_books = [b for b in dic_books]\n",
    "\n",
    "# Indexation\n",
    "ids_users = {}\n",
    "for i in range(len(list_users)):\n",
    "    u = list_users[i]\n",
    "    ids_users[u] = i\n",
    "        \n",
    "ids_books = {}\n",
    "for i in range(len(list_books)):\n",
    "    b = list_books[i]\n",
    "    ids_books[b] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. User-User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It recommends an item to a user if similar users liked this item before. The similarity between two users is computed from the amount of items they have in common in the dataset.\n",
    "\n",
    "This algorithm is very efficient when the number of users is way smaller than the number of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_users(u1, u2):\n",
    "    if u1 == u2:\n",
    "        return 0\n",
    "    else:\n",
    "        s = 0\n",
    "        for b in dic_users[u1]:\n",
    "            if b in dic_users[u2]:\n",
    "                s += 1\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_users(k, u):\n",
    "    k_neighbors = []\n",
    "    \n",
    "    similarities = [similarity_users(u, v) for v in dic_users]\n",
    "    ids = [v for v in dic_users]\n",
    "\n",
    "    for i in range(k): \n",
    "        ind_v = np.argmax(similarities)\n",
    "        v = ids[ind_v]\n",
    "        k_neighbors.append(v)\n",
    "        # Update similarities to make sure a point is not picked twice\n",
    "        similarities[ind_v] = 0\n",
    "    \n",
    "    return k_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_users(k, u, b):\n",
    "    u_neighbors = knn_users(k, u)\n",
    "    \n",
    "    r = 0\n",
    "    for v in u_neighbors:\n",
    "        try:\n",
    "            r += dic_ratings[(b, v)]\n",
    "        except:\n",
    "            k -= 1\n",
    "    \n",
    "    return round(r/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8089887640449438\n",
      "0.6629213483146067\n"
     ]
    }
   ],
   "source": [
    "u = 314\n",
    "n = 100\n",
    "\n",
    "error1 = 0\n",
    "error2 = 0\n",
    "for line in ratings_int[:n]:\n",
    "    b, u, r = line[0], line[1], line[2]\n",
    "    try: \n",
    "        r_pred = prediction_users(50, u, b)\n",
    "        error1 += abs(r-r_pred)\n",
    "        if r != r_pred:\n",
    "            error2 += 1\n",
    "    except:\n",
    "        n -= 1\n",
    "\n",
    "print(error1/n)        \n",
    "print(error2/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Item-Item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It recommends items that are similar to the ones you previously liked. As before the similarity between two items is computed using the amount of users they have in common in the dataset.\n",
    "\n",
    "This algorithm is best when the number of items is way smaller than the number of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_books(b1, b2):\n",
    "    if b1 == b2:\n",
    "        return 0\n",
    "    else:\n",
    "        s = 0\n",
    "        for u in dic_books[b1]:\n",
    "            if u in dic_books[b2]:\n",
    "                s += 1\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_books(k, b):\n",
    "    k_neighbors = []\n",
    "    \n",
    "    similarities = [similarity_books(b, p) for p in dic_books]\n",
    "    ids = [p for p in dic_books]\n",
    "\n",
    "    for i in range(k): \n",
    "        ind_p = np.argmax(similarities)\n",
    "        p = ids[ind_p]\n",
    "        k_neighbors.append(p)\n",
    "        # Update similarities to make sure a point is not picked twice\n",
    "        similarities[ind_p] = 0\n",
    "    \n",
    "    return k_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_books(k, u, b):\n",
    "    b_neighbors = knn_books(k, b)\n",
    "    \n",
    "    r = 0\n",
    "    for p in b_neighbors:\n",
    "        try:\n",
    "            r += dic_ratings[(p, u)]\n",
    "        except:\n",
    "            k -= 1\n",
    "    \n",
    "    return round(r/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5952380952380952\n",
      "0.44047619047619047\n"
     ]
    }
   ],
   "source": [
    "u = 314\n",
    "n = 100\n",
    "\n",
    "error1 = 0\n",
    "error2 = 0\n",
    "for line in ratings_int[:n]:\n",
    "    b, u, r = line[0], line[1], line[2]\n",
    "    try: \n",
    "        r_pred = prediction_books(50, u, b)\n",
    "        error1 += abs(r-r_pred)\n",
    "        if r != r_pred:\n",
    "            error2 += 1\n",
    "    except:\n",
    "        n -= 1\n",
    "\n",
    "print(error1/n)        \n",
    "print(error2/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. User-Item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It combines both approaches to generate recommendations. The simplest ones are based on **matrix factorization** techniques.\n",
    "\n",
    "The factorization can be trained using:\n",
    "- SVD: very computationally intensive\n",
    "- ALS: for medium-scale datasets\n",
    "- SGD: for large-scale datasetsbut, but will be very slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 53424\n",
      "m = 10000\n"
     ]
    }
   ],
   "source": [
    "n = len(list_users)\n",
    "m = len(list_books)\n",
    "\n",
    "print('n = ' + str(n))\n",
    "print('m = ' + str(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Matrix Factorization\n",
    "\n",
    "Source: https://surprise.readthedocs.io/en/stable/matrix_factorization.html\n",
    "\n",
    "Lets note $P \\in R ^{n x k}$ and $Q \\in R ^{k x m}$\n",
    "\n",
    "Optimization problem with regularization term: \n",
    "$$min_{L,R} L(P,Q)$$\n",
    "We have: $$L(P,Q) = \\sum_{(u,i)\\in \\Omega} (r_{ui}-\\hat{r}_{ui})^2 + \\lambda (||p_u||^2 + ||q_i||^2)$$\n",
    "\n",
    "with, \n",
    "- $\\hat{r}_{ui} = q_i^T p_u $\n",
    "- $e_{ui} = r_{ui}-\\hat{r}_{ui}$\n",
    "\n",
    "and $\\Omega$ the set of tuples where the rating already exists "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Matrix Factorization model equation is:\n",
    "\n",
    "$$\\hat{r}_{ij} = \\space u_{i}^T.v_j = \\sum_{k = 0}^K u_{ik} v_{jk}$$ \n",
    "\n",
    "where, <v_u,v_i>\n",
    "- $\\omega_0$ : model bias\n",
    "- $\\omega_u$ : user bias\n",
    "- $\\omega_i$ : item bias\n",
    "- $<v_u,v_i>$ : interaction between user $u$ and item $i$\n",
    "\n",
    "\n",
    "$$ \\text{Loss } = \\sum_{(u_i,v_p, v_n)\\space\\in\\space T} \\max(\\langle u_i,v_{p} \\rangle \\space -  \\space u_i^T.v_{n}+ \\space \\alpha, 0) $$\n",
    "\n",
    "where , $T$ is a set a triplets and\n",
    "- $u_i^T.v_p$ : user to item+ interaction\n",
    "- $u_i^T.v_n$ : user to item- interaction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\hat{r}_{ij} = \\langle u_i,v_j \\rangle = \\sum_{k=1}^K u_{ik}v_{jk}$$\n",
    "\n",
    "$$ \\text{Loss } = \\sum_{(u_i,v_p, v_n)\\space\\in\\space T} \\max(\\langle u_i,v_{p} \\rangle \\space -  \\space \\langle u_i,v_{n} \\rangle + \\space \\alpha, 0) $$\n",
    "\n",
    "$$ \\hat{y}(x) \\space := w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{l=2}^d\\sum_{i_1=1}^n ... \\sum_{i_l=i_{l-1}+1}^n \\left(\\prod_{j=1}^l x_{i_j}\\right) \\left(\\sum_{f=1}^{k_l}\\prod_{j=1}^l  v_{i_j,f}^{(l)}\\right) $$\n",
    "\n",
    "$$ \\hat{y}(x) \\space = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{i=1}^n\\sum_{j=i+1}^n \\langle v_i,v_j \\rangle x_i x_j $$\n",
    "\n",
    "$$ \\hat{y}(x) \\space = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{c,d \\space\\in\\space C} \\sum_{\\substack{i \\space\\in\\space I_c \\\\ j \\space\\in\\space I_d}} \\langle v_i,v_j \\rangle x_i x_j $$\n",
    "\n",
    "$$ \\hat{y}(x) \\space = w_0 + \\sum_{i=1}^n w_i x_i + \\sum_{c,d \\space\\in\\space C} \\lambda_{cd} \\sum_{\\substack{i \\space\\in\\space I_c \\\\ j \\space\\in\\space I_d}} \\langle v_i,v_j \\rangle x_i x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(I_c)_{c \\in C}$$\n",
    "$$I = \\{1, ..., n\\}$$\n",
    "$$\\forall i \\in \\{1, ..., N\\}, \\space x^{(i)} = \\left(x^{(i)}_1, ... , x^{(i)}_n\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this optimization prpoblem we will use a **Gradient descent**.\n",
    "\n",
    "We have:\n",
    "- $p_u \\leftarrow p_u + \\gamma (e_{ui} . q_i - \\lambda p_u)$\n",
    "- $q_i \\leftarrow q_i + \\gamma (e_{ui} . p_u - \\lambda q_i)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_ = 0.005 # learning rate\n",
    "lambda_ = 0.02 # regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norme(V):\n",
    "    return sum([v**2 for v in V.tolist()])\n",
    "\n",
    "def loss(P, Q):\n",
    "    s = 0\n",
    "    for line in ratings_int:\n",
    "        i, u, r_ui = ids_books[line[0]], ids_users[line[1]], line[2]\n",
    "        e_ui = r_ui - np.dot(P[u],Q[i])\n",
    "        s += e_ui ** 2 + lambda_ * (norme(P[u]) + norme(Q[i]))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gradient_descent(k, num_epochs):\n",
    "    #randomly initialize user/item factors from a Gaussian\n",
    "    P = np.random.normal(0,.1,(n, k))\n",
    "    Q = np.random.normal(0,.1,(m, k))\n",
    "    print(P.shape, Q.shape)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for line in ratings_int:\n",
    "            i, u, r_ui = ids_books[line[0]], ids_users[line[1]], line[2]\n",
    "            e_ui = r_ui - np.dot(P[u],Q[i])\n",
    "            \n",
    "            temp = P[u]\n",
    "            P[u] +=  gamma_ * (e_ui * Q[i] - lambda_ * P[u])\n",
    "            Q[i] +=  gamma_ * (e_ui * temp - lambda_ * Q[i])\n",
    "        print(loss(P, Q))\n",
    "\n",
    "    return P, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39999999999999997"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.4/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53424, 10) (10000, 10)\n",
      "15520731.327242896\n",
      "15346211.260263296\n",
      "13659460.476903863\n",
      "9720601.04628521\n",
      "5895851.791168863\n",
      "3609742.9247699957\n",
      "2414261.145094462\n",
      "1779132.5237831143\n",
      "1421231.8366552545\n",
      "1206045.8159536244\n",
      "1069214.676922158\n",
      "978003.8309890237\n",
      "914636.1152599276\n",
      "868936.3220445919\n",
      "834817.6833848476\n",
      "808503.613030002\n",
      "787580.9972283937\n",
      "770469.7758721492\n",
      "756112.7915451486\n",
      "743788.5550889261\n",
      "732995.3719922131\n",
      "723378.1544347245\n",
      "714681.3725723712\n",
      "706718.3019353005\n",
      "699350.579867223\n",
      "692474.3762643394\n",
      "686010.8732956144\n",
      "679899.5993140377\n",
      "674093.686603639\n",
      "668556.4488083773\n",
      "663258.879108588\n",
      "658177.8011217883\n",
      "653294.4893185084\n",
      "648593.6316179949\n",
      "644062.5442536498\n",
      "639690.5745021672\n",
      "635468.6445198099\n",
      "631388.9019418821\n",
      "627444.4517511417\n",
      "623629.1503155715\n",
      "619937.4471729242\n",
      "616364.2635951524\n",
      "612904.899542326\n",
      "609554.9625564455\n",
      "606310.3136117894\n",
      "603167.026056598\n",
      "600121.3546420601\n",
      "597169.7122907954\n",
      "594308.652775053\n",
      "591534.8578709178\n",
      "588845.1278710789\n",
      "586236.3745830146\n",
      "583705.6161371993\n",
      "581249.9730824975\n",
      "578866.6653693354\n",
      "576553.0099188299\n",
      "574306.418552581\n",
      "572124.3961180546\n",
      "570004.5386932788\n",
      "567944.5317898416\n",
      "565942.1485027679\n",
      "563995.2475766767\n",
      "562101.7713735631\n",
      "560259.7437397292\n",
      "558467.2677767143\n",
      "556722.5235274798\n",
      "555023.7655915628\n",
      "553369.3206857765\n",
      "551757.5851666282\n",
      "550187.0225316638\n",
      "548656.1609148027\n",
      "547163.5905910303\n",
      "545707.9615032421\n",
      "544287.9808229745\n",
      "542902.4105555621\n",
      "541550.0651974991\n",
      "540229.8094540568\n",
      "538940.5560221019\n",
      "537681.2634430311\n",
      "536450.9340284715\n",
      "535248.6118617036\n",
      "534073.3808755911\n",
      "532924.3630079135\n",
      "531800.7164331768\n",
      "530701.6338715135\n",
      "529626.3409721305\n",
      "528574.0947706954\n",
      "527544.1822181068\n",
      "526535.9187790096\n",
      "525548.6470977718\n",
      "524581.7357288086\n",
      "523634.57792975905\n",
      "522706.5905141009\n",
      "521797.21276127535\n",
      "520905.90538118314\n",
      "520032.14953101246\n",
      "519175.44588181505\n",
      "518335.3137319426\n",
      "517511.29016573494\n",
      "516702.9292547389\n"
     ]
    }
   ],
   "source": [
    "P, Q = Gradient_descent(10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "0.26\n"
     ]
    }
   ],
   "source": [
    "u = 314\n",
    "n = 100\n",
    "\n",
    "error1 = 0\n",
    "error2 = 0\n",
    "for line in ratings_int[:n]:\n",
    "    i, u, r = ids_books[line[0]], ids_users[line[1]], line[2]\n",
    "    r_pred = round(np.dot(P[u],Q[i]))\n",
    "    error1 += abs(r-r_pred)\n",
    "    if r != r_pred:\n",
    "        error2 += 1\n",
    "\n",
    "print(error1/n)        \n",
    "print(error2/n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**error1** : average distance between the actual rating and the predicted one\n",
    "\n",
    "**error2** : percentage of correctly predicted ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User-User method (k = 50):\n",
    "- error1 = 0.8089887640449438\n",
    "- error2 = 0.6629213483146067\n",
    "\n",
    "Item-Item method (k = 50):\n",
    "- error1 = 0.5952380952380952\n",
    "- error2 = 0.44047619047619047\n",
    "\n",
    "Latent Factorization method (gamma = 0.005, lambda = 0.02):\n",
    "- error1 = 0.47\n",
    "- error2 = 0.39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the **User-User** method has some difficulties to predict the exact ratings, but is able to maintain an average distance between the actual rating and the predicted one smaller than 1. \n",
    "\n",
    "The **Item-Item** method performs better than the User-User method, giving a smaller error for both error1 and error2. It is able to predict correctly the exact rating more the half of the time. \n",
    "\n",
    "Finally, the **Latent Matrix Factorization** method gives the best performance of all. Its results for error1 and error2 are both under 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem** : All these algorithms share the drawback that there is no efficient method to update the embeddings after adding a new item or a new user. This problem is called the **cold-start problem**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
